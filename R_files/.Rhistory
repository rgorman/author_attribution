pbinom(5, 10, .5)
dbinom (1, 10, .5)
dbinom (1, 10, .5) + dbinom (2, 10, .5)
dbinom (1, 10, .5) + dbinom (2, 10, .5) + dbinom (3, 10, .5)
dbinom (1, 10, .5) + dbinom (2, 10, .5) + dbinom (3, 10, .5) + dbinom (4, 10, .5)
dbinom (1, 10, .5) + dbinom (2, 10, .5) + dbinom (3, 10, .5) + dbinom (4, 10, .5) + dbinom (0, 10, .5)
dbinom (1, 10, .5) + dbinom (2, 10, .5) + dbinom (3, 10, .5) + dbinom (4, 10, .5) + dbinom (0, 10, .5) + dbinom (5, 10, .5)
qbinom(.5, 10, .5)
qbinom(.25, 10, .5)
qbinom(.37, 10, .5)
qbinom(.38, 10, .5)
mean (havelaar$Frequency)/ n
qbinom (13, n, .01338)
qbinom (13, 1000, .01338)
pbinom (13, 1000, .01338)
pbinom (21, 1000, .01338)
dbinom (21, 1000, .01338)
dbinom (13, 1000, .01338)
dbinom (30, 1000, .01338)
pbinom (30, 1000, .01338)
qbinom(.38, 10, .5)
qbinom(.38, n, .5)
qbinom(.1, n, .5)
qbinom(.01, n, .5)
mean (havelaar$Frequency)/ n
qbinom(.01, n, .1338384)
qbinom(.02, n, .1338384)
qbinom(.92, n, .1338384)
qbinom(.992, n, .1338384)
qbinom(.992, 1000, .1338384)
qbinom(.992, 10000, .1338384)
qbinom(.992, 100000, .1338384)
qbinom(.992, 100000000, .1338384)
qbinom(.95, 100000000, .1338384)
qbinom(.95, 1000, .1338384)
pbinom (152, 1000, .01338)
qbinom(qnts, 1000, .01338)
quantile(havelaar$Frequency, qnts)
plot (qbinom(qnts, n, .01338), quantile(havelaar$Frequency. qnts))
plot (qbinom(qnts, n, .01338), quantile(havelaar$Frequency, qnts))
plot (qbinom(qnts, n, .01338))
plot (quantile(havelaar$Frequency, qnts))
plot (qbinom(qnts, n, .01338))
plot (qbinom(qnts, n, .01338), quantile(havelaar$Frequency, qnts))
dpois(10, 58856)
dpois(1000, 58856)
dpois(57000, 58856)
dpois(58000, 58856)
dpois(58800, 58856)
plot (dpois(58800, 58856))
havelaar.tab <- xtabs(~ havelaar$Frequency)
havelaar.tab
havelaar.probs <- xtabs(~ havelaar$Frequency)/ nrow(havelaar)
havelaar.probs
havelaar.probs <- xtabs(~ havelaar$Frequency)/ nrow(havelaar) round(havelaar.probs, 3)
havelaar.probs <- xtabs(~ havelaar$Frequency)/ nrow(havelaar) )
havelaar.probs <- xtabs(~ havelaar$Frequency)/ nrow(havelaar)
round (havelaar.probs, 3)
havelaar.probs <- round (havelaar.probs, 3)
sum (havelaar.probs)
plot(as.numeric(names(havelaar.probs)))
plot(as.numeric(names(havelaar.probs)), havelaar.probs)
havelaar.probs <- xtabs(~ havelaar$Frequency)/ nrow(havelaar)
sum (havelaar.probs)
plot(as.numeric(names(havelaar.probs)), havelaar.probs)
plot(as.numeric(names(havelaar.probs)), havelaar.probs, xlim=c(0, 40))
plot(as.numeric(names(havelaar.probs)), havelaar.probs, xlim=c(0, 40), type = "h")
plot (counts)
counts <- 0:42
plot (counts)
plot (counts, dbinom (counts, 1000, .0134))
plot (counts, dbinom (counts, 1000, .0134) type = "h")
plot (counts, dbinom (counts, 1000, .0134), type = "h")
lambda <- 1000*.0134
plot (dpois(counts, lambda))
plot (dpois(counts, lambda), type = "h")
x <- seq(-4, 4, 0.1)
y <- dnorm(x)
plot (x, y)
plot (x, y, xlab="x", ylab="density", ylim = c(0, 0.8), type ="l")
pt(-3, 2)
pt(-3, 20)
27269 %% 40
27240/40
alice [1:10]
alice <- tolower(alice)
wonderland <- data.frame(word = alice[1:27240], chunk = cut (1:27240, breaks 40, labels = F))
wonderland <- data.frame(word = alice[1:27240], chunk = cut (1:27240, breaks = 40, labels = F))
wonderland [1:5,]
wonderland$alice <- wonderland$word == "alice"
wonderland [1:5,]
wonderland$hare <- wonderland$word == "hare"
wonderland$very <- wonderland$word == "very"
wonderland [1:5,]
sum (wonderland$alice)
countOfAlice <- tapply (wonderland$alice, wonderland$chunk, sum)
countOfAlice
countOfAlice.tab <- xtabs(~ countOfAlice)
countOfAlice.tab
countOfHare <-tapply(wonderland$hare, wonderland$chunk, sum)
countOfHare.tab <- xtabs(~ countOfHare)
countOfVery <- tapply (wonderland$very, wonderland$chunk, sum)
countOfVery.tab <- xtabs(~ countOfVery)
plot(countOfAlice.tab)
plot (countOfAlice)
plot (countOfAlice, type = "l")
plot (countOfAlice, type = "h")
plot (countOfHare, type = "h")
plot (countOfvery, type = "h")
plot (countOfVery, type = "h")
countOfAlice
countOfAlice.tab
tappy(countOfAlice.tab, sum)
tapply(countOfAlice.tab, sum)
sum(countOfAlice.tab)
aggregate(countOfAlice.tab, /40)
aggregate(countOfAlice.tab)/40
tapply(countOfAlice, sum)
tapply(as.numeric(names(countOfAlice)), sum)
plot (as.numeric(names(countOfAlice.tab)))
plot (as.numeric(names(countOfAlice.tab)), countOfAlice.tab/sum(countOfAlice.tab))
plot (as.numeric(names(countOfAlice.tab)), countOfAlice.tab/sum(countOfAlice.tab), type = "h", xlim = c(0,18, ylim = c(0,9)))
plot (as.numeric(names(countOfAlice.tab)), countOfAlice.tab/sum(countOfAlice.tab), type = "h", xlim = c(0,18), ylim = c(0,9)))
plot (as.numeric(names(countOfAlice.tab)), countOfAlice.tab/sum(countOfAlice.tab), type = "h", xlim = c(0,18), ylim = c(0,9))
plot (as.numeric(names(countOfAlice.tab)), countOfAlice.tab/40), type = "h", xlim = c(0,18), ylim = c(0,9))
plot (as.numeric(names(countOfAlice.tab)), countOfAlice.tab/40, type = "h", xlim = c(0,18), ylim = c(0,9))
plot (as.numeric(names(countOfAlice.tab)), countOfAlice.tab/40, type = "h", xlim = c(0,18), ylim = c(0, .09))
plot (as.numeric(names(countOfAlice.tab)), countOfAlice.tab/40, type = "h", xlim = c(0,18), ylim = c(0, .5))
plot (as.numeric(names(countOfAlice.tab)), countOfAlice.tab/40, type = "h", xlim = c(0,18), ylim = c(0, 0.5))
density(ver)
density(ver$Frequency)
plot(density(ver$Frequency)
plot(density(ver$Frequency))
plot(density(ver$Ferquency))
plot(density(ver$Frequency))
head (ver)
ver$Frequency <- log(ver$Frequency)
plot(density(ver$Frequency))
log(255)
log (-.5)
exp (-.5)
qqnorm(rnorm(length(ver$Frequency), 4, 3))
abline(v+ qnorm(.025), col = "grey")
abline(v= qnorm(.025), col = "grey")
abline (h= qnorm(.025, 4, 3)col = "grey")
abline(h = qnorm(.025, 4, 3), col = "grey")
head(Ont)
head(durationsOnt)
head(ver)
ver.transp <- ver[ver$SemanticClass == "transparent",]$Frequency
head(ver.transp)
ver.opaque <- ver[ver$SemanticClass == "opaque",]$Frequency
ver.transp.d <- density(ver.transp)
ver.opaque.d <- density(ver.opaque)
ver.transp.d <- density(ver.transp)
ver.opaque.d <- density(ver.opaque)
xlimit = range(ver.trasp.d$x, ver.opaque.d$x)
ylimit = range(ver.transp.d$y, ver.opaque.d$y)
plot(ver.transp.d, lty=1, col ="black", xlab="ferquency", ylab="density", xlim = xlimit, main="")
lines(ver.opaque.d, col = "darkgrey")
ver.transp.d <- density(ver.transp)
ver.opaque.d <- density(ver.opaque)
xlimit = range(ver.trasp.d$x, ver.opaque.d$x)
xlimit = range(ver.transp.d$x, ver.opaque.d$x)
ylimit = range(ver.transp.d$y, ver.opaque.d$y)
plot(ver.transp.d, lty=1, col ="black", xlab="ferquency", ylab="density", xlim = xlimit, main="")
lines(ver.opaque.d, col = "darkgrey")
ks.test(jitter(ver.transp), jitter(ver.opaque))
bwplot(Frequency ~ Class | Complex, data=ratings)
library(lattice)
bwplot(Frequency ~ Class | Complex, data=ratings)
q
q()
library(XML)
source("code/corpusFunctions.R")
input.dir <- "data/relationPosFiles"
files.v <- dir(path=input.dir, pattern=".*xml")
input.dir
library(stylo)
stylo.default.settings()
f <- c("Homer", "Hesiod", "Plutarch")
rep (f, 5)
factor(rep(f, 5))
cl <- factor(rep(f, 5))
is.factor(cl)
setwd("C:/data/Bob/TextAnalysisInR/TextAnalysisWithR")
setwd("C:/authorship_attribution_working/R_files")
input.dir <- "../sWord_output/sWord_relation_files"
files.v <- dir(path=input.dir, pattern=".*xml")
library(XML)
source("code/corpusFunctions.R")
x <- gsub (".xml", "", files.v)
x
names_for_files.v <- gsub (".xml", "", files.v)
x <-NULL
i <- 1
doc.object <- xmlTreeParse(file.path(input.dir, files.v[i]), useInternalNodes=TRUE)
clas(doc.object)
class(doc.object)
swords <-getNodeSet(doc.object, "//sWord", )
sword.content <- paste(sapply(swords, xmlValue), collapes = " ")
sword.content.lower <- tolower(sword.content)
book.freqs.t <-table(sword.content.lower)
class(books.freq.t)
class(books.freqs.t)
class(book.freqs.t)
summary(book.freqs.t)
book.freqs.t[2]
book.freqs.rel.t <- 100*(book.freqs.t/sum(book.freqs.t))
book.freqs.rel.t [1:10]
write.csv(book.freqs.rel.t, file="sWord_output/rel_csv/file1.csv")
h <- c("sWord_output/rel_csv/", names_for_files[1])
h <- c("sWord_output/rel_csv/", names_for_files.v[1])
h
output_dir.v <- "sWord_output/rel_csv/"
names_for_output.v <- gsub (".xml", ".csv", files.v)
output_path <- append(output_dir.v, names_for_output.v[i])
output_path
output_path <- append(output_dir.v, names_for_output.v[i], after(length(output_dir.v)))
output_path <- append(output_dir.v, names_for_output.v[i], after=length(output_dir.v))
output_path
output_path <- paste(output_dir.v, names_for_output.v[i], sep="", collapse="NULL")
output_path
t(book.freqs.rel.t)
csv_ready.t <-t(book.freqs.rel.t)
csv_erady.t [1:20]
csv_ready.t [1:20]
write.csv(csv_ready.t, file=output_path)
row.names(csv_ready.t) <-  names_for_output.v[i]
csv_ready.t [1:20]
t
output_path <- paste(output_dir.v, names_for_output.v[i], sep="", collapse="NULL")
write.csv(csv_ready.t, file=output_path)
row.names(csv_ready.t) <-  names_for_files.v[i]
output_path <- paste(output_dir.v, names_for_output.v[i], sep="", collapse="NULL")
write.csv(csv_ready.t, file=output_path)
library(XML)
source("code/corpusFunctions.R")
#set input directory for desired files
input.dir <- "../sWord_output/sWord_relation_files"
files.v <- dir(path=input.dir, pattern=".*xml")
names_for_files.v <- gsub (".xml", "", files.v)
names_for_output.v <- gsub (".xml", ".csv", files.v)
output_dir.v <- "sWord_output/rel_csv/"
for (i in length(files.v)) {
#read xml file into doc.object
doc.object <- xmlTreeParse(file.path(input.dir, files.v[i]), useInternalNodes=TRUE)
# create R object of the target nodes in xml file
# be sure to select the kind of node you want here!
swords <-getNodeSet(doc.object, "//sWord", )
# create character vector of all values of target xml nodes
sword.content <- paste(sapply(swords, xmlValue), collapes = " ")
# change contents of character vector to lower case characters
sword.content.lower <- tolower(sword.content)
# make relative frequency table of count of like elements in character vector
book.freqs.t <-table(sword.content.lower)
# change frequency table to relative frequency giving each feature as % of total
book.freqs.rel.t <- 100*(book.freqs.t/sum(book.freqs.t))
# transpose relative frequency table so that rows are observations and columns are features.
# R prefers this configuration.
csv_ready.t <-t(book.freqs.rel.t)
# Add file name as row name for relative frequency table
row.names(csv_ready.t) <-  names_for_files.v[i]
# create correct path and file name for output
output_path <- paste(output_dir.v, names_for_output.v[i], sep="", collapse="NULL")
#write relative frequency table to disk as csv file
write.csv(csv_ready.t, file=output_path)
}
doc.object <- xmlTreeParse(file.path(input.dir, files.v[1]), useInternalNodes=TRUE)
swords <-getNodeSet(doc.object, "//sWord", )
sword.content <- paste(sapply(swords, xmlValue), collapes = " ")
sword.content.lower <- tolower(sword.content)
book.freqs.t <-table(sword.content.lower)
book.freqs.rel.t <- 100*(book.freqs.t/sum(book.freqs.t))
csv_ready.t <-t(book.freqs.rel.t)
row.names(csv_ready.t) <-  names_for_files.v[i]
output_path <- paste(output_dir.v, names_for_output.v[i], sep="", collapse="NULL")
write.csv(csv_ready.t, file=output_path)
{
#read xml file into doc.object
doc.object <- xmlTreeParse(file.path(input.dir, files.v[1]), useInternalNodes=TRUE)
# create R object of the target nodes in xml file
# be sure to select the kind of node you want here!
swords <-getNodeSet(doc.object, "//sWord", )
# create character vector of all values of target xml nodes
sword.content <- paste(sapply(swords, xmlValue), collapes = " ")
# change contents of character vector to lower case characters
sword.content.lower <- tolower(sword.content)
# make relative frequency table of count of like elements in character vector
book.freqs.t <-table(sword.content.lower)
# change frequency table to relative frequency giving each feature as % of total
book.freqs.rel.t <- 100*(book.freqs.t/sum(book.freqs.t))
# transpose relative frequency table so that rows are observations and columns are features.
# R prefers this configuration.
csv_ready.t <-t(book.freqs.rel.t)
# Add file name as row name for relative frequency table
row.names(csv_ready.t) <-  names_for_files.v[i]
# create correct path and file name for output
output_path <- paste(output_dir.v, names_for_output.v[i], sep="", collapse="NULL")
#write relative frequency table to disk as csv file
write.csv(csv_ready.t, file=output_path)
}
input.dir <- "../sWord_output/sWord_relation_files"
files.v <- dir(path=input.dir, pattern=".*xml")
names_for_files.v <- gsub (".xml", "", files.v)
names_for_output.v <- gsub (".xml", ".csv", files.v)
output_dir.v <- "sWord_output/rel_csv/"
for (i = 1 {
doc.object <- xmlTreeParse(file.path(input.dir, files.v[1]), useInternalNodes=TRUE)
input.dir <- "../sWord_output/sWord_relation_files"
files.v <- dir(path=input.dir, pattern=".*xml")
names_for_files.v <- gsub (".xml", "", files.v)
names_for_output.v <- gsub (".xml", ".csv", files.v)
output_dir.v <- "sWord_output/rel_csv/"
i <- 1
doc.object <- xmlTreeParse(file.path(input.dir, files.v[i]), useInternalNodes=TRUE)
swords <-getNodeSet(doc.object, "//sWord", )
sword.content <- paste(sapply(swords, xmlValue), collapes = " ")
sword.content.lower <- tolower(sword.content)
book.freqs.t <-table(sword.content.lower)
book.freqs.rel.t <- 100*(book.freqs.t/sum(book.freqs.t))
csv_ready.t <-t(book.freqs.rel.t)
row.names(csv_ready.t) <-  names_for_files.v[i]
output_path <- paste(output_dir.v, names_for_output.v[i], sep="", collapse="NULL")
write.csv(csv_ready.t, file=output_path)
i <- 2
doc.object <- xmlTreeParse(file.path(input.dir, files.v[i]), useInternalNodes=TRUE)
swords <-getNodeSet(doc.object, "//sWord", )
sword.content <- paste(sapply(swords, xmlValue), collapes = " ")
sword.content.lower <- tolower(sword.content)
book.freqs.t <-table(sword.content.lower)
book.freqs.rel.t <- 100*(book.freqs.t/sum(book.freqs.t))
csv_ready.t <-t(book.freqs.rel.t)
row.names(csv_ready.t) <-  names_for_files.v[i]
output_path <- paste(output_dir.v, names_for_output.v[i], sep="", collapse="NULL")
write.csv(csv_ready.t, file=output_path)
i <- NULL
input.dir <- "../sWord_output/sWord_relation_files"
files.v <- dir(path=input.dir, pattern=".*xml")
names_for_files.v <- gsub (".xml", "", files.v)
names_for_output.v <- gsub (".xml", ".csv", files.v)
output_dir.v <- "sWord_output/rel_csv/"
i <- NULL
for (i in length(files.v) {
#read xml file into doc.object
doc.object <- xmlTreeParse(file.path(input.dir, files.v[i]), useInternalNodes=TRUE)
# create R object of the target nodes in xml file
# be sure to select the kind of node you want here!
swords <-getNodeSet(doc.object, "//sWord", )
# create character vector of all values of target xml nodes
sword.content <- paste(sapply(swords, xmlValue), collapes = " ")
# change contents of character vector to lower case characters
sword.content.lower <- tolower(sword.content)
# make relative frequency table of count of like elements in character vector
book.freqs.t <-table(sword.content.lower)
# change frequency table to relative frequency giving each feature as % of total
book.freqs.rel.t <- 100*(book.freqs.t/sum(book.freqs.t))
# transpose relative frequency table so that rows are observations and columns are features.
# R prefers this configuration.
csv_ready.t <-t(book.freqs.rel.t)
# Add file name as row name for relative frequency table
row.names(csv_ready.t) <-  names_for_files.v[i]
# create correct path and file name for output
output_path <- paste(output_dir.v, names_for_output.v[i], sep="", collapse="NULL")
#write relative frequency table to disk as csv file
write.csv(csv_ready.t, file=output_path)
}
input.dir <- "../sWord_output/sWord_relation_files"
files.v <- dir(path=input.dir, pattern=".*xml")
names_for_files.v <- gsub (".xml", "", files.v)
names_for_output.v <- gsub (".xml", ".csv", files.v)
output_dir.v <- "sWord_output/rel_csv/"
i <- NULL
for (i 1:length(files.v) {
#read xml file into doc.object
doc.object <- xmlTreeParse(file.path(input.dir, files.v[i]), useInternalNodes=TRUE)
# create R object of the target nodes in xml file
# be sure to select the kind of node you want here!
swords <-getNodeSet(doc.object, "//sWord", )
# create character vector of all values of target xml nodes
sword.content <- paste(sapply(swords, xmlValue), collapes = " ")
# change contents of character vector to lower case characters
sword.content.lower <- tolower(sword.content)
# make relative frequency table of count of like elements in character vector
book.freqs.t <-table(sword.content.lower)
# change frequency table to relative frequency giving each feature as % of total
book.freqs.rel.t <- 100*(book.freqs.t/sum(book.freqs.t))
# transpose relative frequency table so that rows are observations and columns are features.
# R prefers this configuration.
csv_ready.t <-t(book.freqs.rel.t)
# Add file name as row name for relative frequency table
row.names(csv_ready.t) <-  names_for_files.v[i]
# create correct path and file name for output
output_path <- paste(output_dir.v, names_for_output.v[i], sep="", collapse="NULL")
#write relative frequency table to disk as csv file
write.csv(csv_ready.t, file=output_path)
}
input.dir <- "../sWord_output/sWord_relation_files"
files.v <- dir(path=input.dir, pattern=".*xml")
names_for_files.v <- gsub (".xml", "", files.v)
names_for_output.v <- gsub (".xml", ".csv", files.v)
output_dir.v <- "sWord_output/rel_csv/"
#loop to process files
for (i in 1:length(files.v) {
#read xml file into doc.object
doc.object <- xmlTreeParse(file.path(input.dir, files.v[i]), useInternalNodes=TRUE)
# create R object of the target nodes in xml file
# be sure to select the kind of node you want here!
swords <-getNodeSet(doc.object, "//sWord", )
# create character vector of all values of target xml nodes
sword.content <- paste(sapply(swords, xmlValue), collapes = " ")
# change contents of character vector to lower case characters
sword.content.lower <- tolower(sword.content)
# make relative frequency table of count of like elements in character vector
book.freqs.t <-table(sword.content.lower)
# change frequency table to relative frequency giving each feature as % of total
book.freqs.rel.t <- 100*(book.freqs.t/sum(book.freqs.t))
# transpose relative frequency table so that rows are observations and columns are features.
# R prefers this configuration.
csv_ready.t <-t(book.freqs.rel.t)
# Add file name as row name for relative frequency table
row.names(csv_ready.t) <-  names_for_files.v[i]
# create correct path and file name for output
output_path <- paste(output_dir.v, names_for_output.v[i], sep="", collapse="NULL")
#write relative frequency table to disk as csv file
write.csv(csv_ready.t, file=output_path)
}
for (i in 1:length(files.v)) {
#read xml file into doc.object
doc.object <- xmlTreeParse(file.path(input.dir, files.v[i]), useInternalNodes=TRUE)
# create R object of the target nodes in xml file
# be sure to select the kind of node you want here!
swords <-getNodeSet(doc.object, "//sWord", )
# create character vector of all values of target xml nodes
sword.content <- paste(sapply(swords, xmlValue), collapes = " ")
# change contents of character vector to lower case characters
sword.content.lower <- tolower(sword.content)
# make relative frequency table of count of like elements in character vector
book.freqs.t <-table(sword.content.lower)
# change frequency table to relative frequency giving each feature as % of total
book.freqs.rel.t <- 100*(book.freqs.t/sum(book.freqs.t))
# transpose relative frequency table so that rows are observations and columns are features.
# R prefers this configuration.
csv_ready.t <-t(book.freqs.rel.t)
# Add file name as row name for relative frequency table
row.names(csv_ready.t) <-  names_for_files.v[i]
# create correct path and file name for output
output_path <- paste(output_dir.v, names_for_output.v[i], sep="", collapse="NULL")
#write relative frequency table to disk as csv file
write.csv(csv_ready.t, file=output_path)
}
input.dir <- "../sWord_output/Rel_pos_files"
files.v <- dir(path=input.dir, pattern=".*xml")
names_for_files.v <- gsub (".xml", "", files.v)
names_for_output.v <- gsub (".xml", ".csv", files.v)
output_dir.v <- "sWord_output/rel_pos_csv/"
for (i in 1:length(files.v)) {
#read xml file into doc.object
doc.object <- xmlTreeParse(file.path(input.dir, files.v[i]), useInternalNodes=TRUE)
# create R object of the target nodes in xml file
# be sure to select the kind of node you want here!
swords <-getNodeSet(doc.object, "//sWord", )
# create character vector of all values of target xml nodes
sword.content <- paste(sapply(swords, xmlValue), collapes = " ")
# change contents of character vector to lower case characters
sword.content.lower <- tolower(sword.content)
# make relative frequency table of count of like elements in character vector
book.freqs.t <-table(sword.content.lower)
# change frequency table to relative frequency giving each feature as % of total
book.freqs.rel.t <- 100*(book.freqs.t/sum(book.freqs.t))
# transpose relative frequency table so that rows are observations and columns are features.
# R prefers this configuration.
csv_ready.t <-t(book.freqs.rel.t)
# Add file name as row name for relative frequency table
row.names(csv_ready.t) <-  names_for_files.v[i]
# create correct path and file name for output
output_path <- paste(output_dir.v, names_for_output.v[i], sep="", collapse="NULL")
#write relative frequency table to disk as csv file
write.csv(csv_ready.t, file=output_path)
}
input.dir <- "../sWord_output/Rel_pos_DD_files"
files.v <- dir(path=input.dir, pattern=".*xml")
input.dir <- "../sWord_output/Rel_pos_DD"
files.v <- dir(path=input.dir, pattern=".*xml")
names_for_files.v <- gsub (".xml", "", files.v)
names_for_output.v <- gsub (".xml", ".csv", files.v)
output_dir.v <- "sWord_output/rel_pos_dd_csv/"
#loop to process files
for (i in 1:length(files.v)) {
#read xml file into doc.object
doc.object <- xmlTreeParse(file.path(input.dir, files.v[i]), useInternalNodes=TRUE)
# create R object of the target nodes in xml file
# be sure to select the kind of node you want here!
swords <-getNodeSet(doc.object, "//sWord", )
# create character vector of all values of target xml nodes
sword.content <- paste(sapply(swords, xmlValue), collapes = " ")
# change contents of character vector to lower case characters
sword.content.lower <- tolower(sword.content)
# make relative frequency table of count of like elements in character vector
book.freqs.t <-table(sword.content.lower)
# change frequency table to relative frequency giving each feature as % of total
book.freqs.rel.t <- 100*(book.freqs.t/sum(book.freqs.t))
# transpose relative frequency table so that rows are observations and columns are features.
# R prefers this configuration.
csv_ready.t <-t(book.freqs.rel.t)
# Add file name as row name for relative frequency table
row.names(csv_ready.t) <-  names_for_files.v[i]
# create correct path and file name for output
output_path <- paste(output_dir.v, names_for_output.v[i], sep="", collapse="NULL")
#write relative frequency table to disk as csv file
write.csv(csv_ready.t, file=output_path)
}
